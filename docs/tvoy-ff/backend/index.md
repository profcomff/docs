# Backend-разработка

> **Note**
> Документации по всем API можно посмотреть тут: https://api.profcomff.com/

Бэкенд — начинка сайта или приложения, скрытая от пользователя. Бэкендом называют программно-аппаратную часть сервиса, которая работает на сервере, а не в браузере или на компьютере.

## Зачем нужен?
Слой бэкенда нужен для того, чтобы:
1. Упростить разнесение обновлений до конкретных пользователей: бэкенд разворачивается на наших серверах, где мы контролируем обновления
2. Скрыть чувствительные данные от пользователей
3. Упростить работу пользовательского устройства, снизить требования к его железу
4. Разделить UI и сложные алгоритмы, продиктованные бизнес логикой

## Разработка
В нашей команде разработка ведется с использованием `python 3.11`. Используемые библиотеки: `Pydantic` - для сериализации и десиарелизации данных, `FastAPI` - HTTP-сервер, `SQLAlchemy` - для работы с базами данных, `alembic` - для генерации миграций баз данных, `black` и `isort` - для форматирования кода, `pytest` - для тестирования кода.

Используемые технологии: `PostgreSQL`, `Kafka`, `Docker`

Все зависимости прописываются в файле `requirements.txt` без указания версий (для того, чтобы они автоматически скачивали самую новую). Разработческие зависимости можно прописывать в `requirements.dev.txt`. Там мы точно указываем [black](https://pypi.org/project/black/), [isort](https://pypi.org/project/isort/). Если вы склонировали один из наших проектов, то после настройки конфигурации запуска и создания venv, стоит запустить `pip install -m requirements.txt -r requirements.dev.txt`. После этого все зависимости установятся.

## Как работаем с конфигом
Для конфигурации мы используем переменные окружения. Чтобы многократно не заполнять них, мы используем файлы окружения. Для работы с файлами окружения мы используем ``Pydantic-settings``. В файле settings.py создается класс Settings extends BaseSettings, в котором определятся, что должно быть в .env файле. Здесь и везде далее используются type hints. Сам python не строго относится к их соблюдения, только подсказывает вам в IDE, что вы должны передать в качестве аргументов куда-либо, где указаны типы. Однако Pydantic проверяет соответствие указанных типов и полученных данных, что очень удобно и безопасно.
https://fastapi.tiangolo.com/advanced/settings/#pydantic-settings

Локально создается `.env` файл в корне проекта. То есть, он будет виден в рабочем каталоге как `.env`, а не как `../.env` и прочие вариации путей до него. То есть в файле `settings.py` прописывается что то такое:
```python
from pydantic import ConfigDict, PostgresDsn
from pydantic_settings import BaseSettings


class Settings(BaseSettings):
    """Application settings"""

    DB_DSN: PostgresDsn = 'postgresql://postgres@localhost:5432/postgres'
    model_config = ConfigDict(case_sensitive=True, env_file=".env", extra="allow")
```

Пример .env файла, например, из бэкенда расписания: https://github.com/profcomff/timetable-backend/blob/main/.env.example.
Когда вы первый раз отправляете коммит, проверяйте наличие файла `.gitignore`, `.env` файл не должен попасть в изменения.

## Как работаем с базой данных
Основной базой данных у нас является `PostgreSQL`

Для работы с БД мы используем `SQLAlchemy`. Это очень мощный инструмент разработки с встроенным ORM. Смысл ORM - установка соответствий между моделями с БД и объектами в python. Таким образом, вы работаете с экземплярами соответствующих классов, а не с чистыми SQL-запросами, которые зачастую очень громоздкие и неудобные в дальнейшей работе с ними.

Для начала, создаются все модели(классы) в python. Путь к ним обычно выглядит как /models/db.py. Поля таких классов - колонки. При создании таблиц `sqlalchemy.mapped_column` создает нужную колонку, а в дальнейшем при обращении к этому полю, выдает значение для текущей строки в БД.
https://docs.sqlalchemy.org/en/20/orm/quickstart.html#declare-models

После создания моделей, описываются `relationships` между ними. Это нужно для простой и быстрой реализации больших и, кажется, ненужных запросов. То есть, `childrens = session.query(Parent).filter(Children.parent_id == parent.id).all()` превращается в `parent.childrens`. Очень упрощает читаемость кода.
https://docs.sqlalchemy.org/en/20/orm/basic_relationships.html

Отношения между моделями бывают разные: one-to-one, one-to-many, many-to-many. Все их реализации достаточно просты, но в любом случае требуют изучения:
1. <https://docs.sqlalchemy.org/en/20/orm/basic_relationships.html#one-to-many>
2. <https://docs.sqlalchemy.org/en/20/orm/basic_relationships.html#one-to-one>
3. <https://docs.sqlalchemy.org/en/20/orm/basic_relationships.html#many-to-many>

В процессе разработки структура БД может меняться, для того, чтобы это все автоматически раскатывалась на сервере, мы используем alembic.  __Любые__ изменения в БД должны сопровождаться генерацией __нового__ файла миграций. Ваша БД должна содержать только те таблицы, которые нужны в данном проекте.

__Бесполезно изменять файл уже существующий файл миграций__

__При изменении моделей БД__:
1. Запустить:
    ```bash
    alembic revision --autogenerate -m "issue_number_and_name"
    ```
2. Проверьте, что в папке `migrations/versions` создался новый файлик и его содержимое соответствует вашим ожиданиям
    - Если что-то сформировалось не так, обязательно поправьте. Обычно переименования колонок или таблиц алембиком воспринимаются как удаление и создание новых, это ошибка!
    - Запустить автоформатирование файла с помощью `black` и `isort`, чтоб делать файлики миграций красивыми
3. Запустить,  для применения миграций:
    ```bash
    alembic upgrade head
    ```

## Как работаем с вебом

Для работы с вебом мы используем __FastAPI__.

Работаем мы по стандартам REST API: https://habr.com/ru/post/483202/

Для каждой сущности создается свой собственный роутер, в котором минимум указывается `prefix` и `tags`. `prefix` - это то, что будет всегда стоять в начале пути для данного роутера, `tags` - для удобства чтения документации в Swagger UI. 

Роутер - это объединение ручек по признаку принадлежности к одному ресурсу.

### Интерфейс API

REST API состоит из нескольких типов ручек: GET/POST/PATCH/DELETE.
- __GET__ запрос не может иметь тело в виде json. Его __не__ стоит использовать для передачи конфиденциальных данных, т.к браузер может сохранять историю запросов и т.д. Запрос будет выглядеть так `GET /router/{id}` для конкретного ресурса и `GET /router` для всех

- __POST__ запрос уже может содержать тело в виде json. Его используют для создания ресурса. JSON передается в виде __верифицируемой__ модели `pydantic`. То есть, описывается класс с полями, которые должен содержать передаваемый json. Опять же указываются type hints и необходимость передачи того или иного поля (например `id: int | None`). Запрос будет выглядеть так: `POST /router`

- __PATCH__ запрос во многом похож на POST (не рассматриваемый здесь PUT туда же). Используется для редактирования ресурса. Во многом это просто создано для удобства, по факту можно использовать для обновления и POST запросы. Но это противоречит спецификации REST, так что мы так не делаем. Запрос будет выглядеть так: `PATCH /router/{id}`

- __DELETE__ аналогичен, используется для удаления ресурсов. Вместо него так же можно использовать другие запросы с постфиксами /delete, но так как мы работаем по REST API, мы используем этот тип запросов. Пример запроса: `DELETE /router/{id}`

__Каждый__ роутер складывается в __отдельный__ файл, потом они собираются вместе в файле `base.py` через функцию `FastAPI().include_router()`. Все роутеры лежат в директории /routes/ вместе с base.py.

Когда ручка что то возвращает, мы используем json, __верифицируемый__ Pydantic'ом. Аналогично передаче данных в запрос, описанной выше:  описывается класс с полями, которые должен содержать получаемый json. Опять же указываются type hints и необходимость содержания того или иного поля(например `id: int | None`) в ответе.

### Простое FastAPI приложение
Выгляит примерно так:
```python
from fastapi import FastAPI

app = FastAPI()


@app.get("/example")
async def root():
    return {"message": "Hello World"}
```

Здесь прописано, что надо создать само приложение, а также положить по адресу `http://<host>/example`, ручку, которая будет на любой запрос отдавать:
```json
{"message": "Hello World"}
```

Более детально каждая строка описана тут: <https://fastapi.tiangolo.com/tutorial/first-steps/>

### Где искать документацию по ручкам
После того, как вы запустили FastAPI App, по адресу `http://<host>/docs` вас будет ждать _интерактивная документация_. Из нее можно кидать запросы, смотреть ответы.

Данный шаг отмечен тут: <https://fastapi.tiangolo.com/tutorial/first-steps/#interactive-api-docs>

### Пагинация
Если запрос может возвращать __большое__ количество данных, используется пагинация: ответ от сервера в таком случае представляет из себя: `{"items": result, "limit": limit, "offset": offset, "total": result.count()}`, где `limit` - максимальный размер ответа, `offset` - смещение от первого элемента, `total` - количество подходящих записей в БД.

### Middlewares
Это функция, часть которой будет вызвана __до__ запроса, часть __после__. Таким образом, можно управлять каждым запросом, добавлять информацию про него куда нибудь, добавлять какие то поля внутрь него, оборачивать в транзакции и т.д

В `base.py` подключаются __middlewares__. Про CORS, используемый, когда фронт лежит отдельно от бэка можно почитать тут: <https://docs.profcomff.com/tvoy-ff/backend/settings.html>

Подключаемый `DBSessionMiddleware` нужен для создания сессии работы с БД прямо в FastAPI, он содержится в библиотеке [fastapi-sqlalchemy](https://pypi.org/project/FastAPI-SQLAlchemy/).

Также, можно подключать и собственные middlewares, как это делать можно посмотреть тут: <https://fastapi.tiangolo.com/tutorial/middleware/>

### Pydantic
Фреймворк для сериализации и десериализации данных.

Сериализация - это процесс преобразования сложных структур данных, таких как объекты, массивы, словари и т.д., в более простой формат, который может быть сохранен в файле или передан через сеть. Сериализация используется для сохранения состояния объектов или передачи данных между программами. Результатом сериализации может быть текстовая строка или бинарное представление, которое легко восстановить обратно в исходные данные.

Десериализация - это обратный процесс, при котором данные, ранее сериализованные, восстанавливаются в исходную сложную структуру данных. Десериализация выполняется с целью использования или анализа этих данных в программе.

Например, сериализация переводит объект класса в строку, а десериализация - наоборот.

Он тесно связан с `FastAPI`, использвуется там для указания типов входных данных.

Оссновная сущность - _модель_. Выглядит так:

```python
class ParamGet(ParamPost):
    id: int
    category_id: int
    comment: str
```

Данная модель создана для того, чтобы сериализовать и десериализовать `json` вида:
```json
{
    "id": 2, 
    "category_id": 5,
    "comment": "Hello, world!"
}
```

Если в FastAPI пришел json не этого формата, то он _автоматически_ выбросит ошибку 429 - Validation Error.

Модели могут ссылаться друг на друга, тогда получается вложенная структура json.

```python
class ParamGet(ParamPost):
    id: int
    category_id: int
    comment: str


class CategoryGet(CategoryPost):
    id: int
    params: list[ParamGet] | None = None
```

Таким образом, получается вложенная структура вида:
```json
{
    "id": 4,
    "params": [
                {
                "id": 2, 
                "category_id": 4,
                "comment": "Hello, world!"
                },
                {
                "id": 3, 
                "category_id": 4,
                "comment": "Hello, profcomff!"
                }
    ]
}
```

Также, в `pydantic` есть валидаторы данных, представляемые в виде вызываемых в момент сериализации и десериализации функций. Почитать об этом можно тут: <https://docs.pydantic.dev/latest/concepts/validators/#before-after-wrap-and-plain-validators>

## Структура проекта
- `./github` и лежащая в ней `./github/workflows`: папки, где лежат папйплайны CI/CD. Что такое CI/CD можно почитать тут: <https://habr.com/ru/companies/otus/articles/515078/>. Эта папка вам нужна, если вы хотите добавить автоматический запуск на какое то событие в GitHub какого то действия. Например, запуск тестов на каждый Pull request
- `LICENSE` - файл с лицензией на открытый код
- `README.md` - документация к проекту
- `pyproject.toml` - конфигурация проекта Python. Что это такое: можно почитать тут: <https://packaging.python.org/en/latest/guides/writing-pyproject-toml/>
- `.gitignore` - файл с описанием того, что должен игнорировать git при коммитах
- `Dockerfile` - файл с описанием сборки проекта для запуска его в docker контейнере
- `requirements.txt`, `requirements.dev.txt` - файлы зависимостей
- Сам проект, папка с исходным кодом
- Папка с тестами
- Папка с миграциями для БД

## Как сделать Pull request

Для начала, вы должны помнить следующее:

1. __Все__ пароли, Redierct URLs и прочее указываются в `.env` файле и подгружается в класс `Settings`. Мы не храним пароли и адреса наших серверов в коде.

2. Кроме `requirements.txt` в корне проекта лежат файлы: `.env`, `.gitignore`, `flake8.conf`, `pyproject.toml`, `LICENSE`. Это все файлы конфигураций и файлы для GitHub - вообще относительно разработческие штуки. Их можно взять в любом готовом проекте (кроме `.env`)

3. Если вы создаете PR в пустом репозитории: В корне проекта создается __исполняемый__ модуль и модуль __тестов__, папка с миграциями, путь `/.github/workflows/`. В каждой директории модулей должен лежать файл `__init__.py`, а в директории исполняемого модуля должен лежать и файл `__main__.py`. Запускать надо исполняемый модуль (`python3 -m...`)

4. __Любые__ изменения в БД должны сопровождаться генерацией файла миграций. Ваша БД должна содержать только те таблицы, которые нужны в данном проекте.

5. __Любые__ изменения в структуре проекта должны быть прописанными в `Dockerfile`, __если они его затронули__. При создании папок, если они пустые, создавайте там пустой файл `.gitkeep`, иначе папка может не закоммититься. После этого не забудьте прописать этот путь в `Dockerfile`.

### __Итак__, чтобы сделать Pull request, вам надо:

1. Создать ветку в GitHub, работать в ней
2. Написать код, решающий задачу
3. Написать тесты к вашему коду
4. Проверить, сгенерирован ли файл миграций, если изменена структура БД
5. Проверить, все ли папки, созданные вами, закоммитились в вашу ветку. Проверить, добавили ли вы их в Dockerfile
6. Проверить соответствие проекта стандартам PEP8, прогнать по коду black
7. Запросить review у любого из старших разработчиков
8. Исправить ошибки, если присутствуют на этапе review
9. Merge! Вы великолепны.

## Как создать новый проект
1. Создайте репозиторий, можете инициализировать его первым коммитом с `README.md` (вам предложит это GitHub)
2. Сделайте все действия, описанные тут: <https://github.com/profcomff/fastapi-template>
3. Закоммитьте изменения прямо в мастер
4. Создайте feature ветку и делайте там проект

## Выкатываем новый бэкэнд на прод
Чтобы новый сервис появился в тесте и проде нужно сделать несколько важных вещей:

1. Подготовить БД (это надо повторить и в тестовой БД, и в продовой)
    1. Создать новых пользователей в тестовой и продовой базах данных: `CREATE USER srvc_test_marketing_api IN GROUP group_service_test PASSWORD '...';`
    2. Создать новые схемы для хранения таблиц: `create schema api_marketing authorization srvc_test_marketing_api;`
    3. Назначить для пользователя схему по умолчанию: `alter user srvc_test_marketing_api set search_path to api_marketing;`
2. Настройка репозитория
    1. Создать среды исполнения (Environments) для теста и прода (обычно во всех репозиториях Testing и Production)
    2. В средах создать переменные с ключами, необходимые для запуска сервиса. Например данные для подключения к БД мы кладем в переменную `DB_DSN`: <br/> <img width="864" alt="image" src="https://user-images.githubusercontent.com/5656720/192113572-cf417d78-1868-4f36-99d9-14d2b67fa0a2.png"> <br/> *На изображении 2 ключа, к которым можно [обратиться из CI через](https://github.com/profcomff/marketing-backend/blob/main/.github/workflows/build_and_publish.yml#L115) `{{ secrets.НАЗВАНИЕ }}`*
    3. На прод настроить ревьюера, который сможет выкатывать сервис для всех пользователей.
3. Упаковать проект для запуска в Docker
    1. Создать `Dockerfile` в корне проекта. [Пример докерфайла](https://github.com/profcomff/marketing-backend/blob/main/Dockerfile)
4. Настроить GitHub Actions для автоматического запуска сборки и запуска на сервере
    1. Создать папку `.github/workflows`
    2. Положить в него файлики для раскатки теста, прода, тестирования и т.д.
    3. В качестве шаблона можно использовать [этот пример](https://github.com/profcomff/marketing-backend/blob/main/.github/workflows/build_and_publish.yml). Тут происходит раскатка в тест при коммитах в ветку `main` и раскатка в прод при создании тегов.
5. Настроить сервер
    1. Сервисы находятся в отдельных докер контейнерах, их видно только во внутренней сети
    2. У нас есть Caddy (это [reverse proxy](https://ru.wikipedia.org/wiki/%D0%9E%D0%B1%D1%80%D0%B0%D1%82%D0%BD%D1%8B%D0%B9_%D0%BF%D1%80%D0%BE%D0%BA%D1%81%D0%B8) [http сервер](https://ru.wikipedia.org/wiki/%D0%92%D0%B5%D0%B1-%D1%81%D0%B5%D1%80%D0%B2%D0%B5%D1%80)), который запросы из внешней сети прокидывает во внутреннюю. В него надо добавить новую запись [reverse_proxy](https://caddyserver.com/docs/caddyfile/directives/reverse_proxy).

    Пример Caddy записи
    ```
    printer.api.profcomff.com:443 {
        reverse_proxy com_profcomff_api_printer:80
    }
    ```
    3. Понять, что по ссылкам в прошлом пункте ничего не понятно и просто скопировать готовую конфигурацию соседнего сервиса :)


## Проверяем корректность миграций
1. Просим админов БД скинуть нужный дамп (получить его можно только если есть соответствующие доступы)
Команда для дампа: `pg_dump -U {role} -h {host} -p {port} -d {db_name} -n {schema_name} > /tmp/{name}.dump`
2. Кидаем его в БД:
`psql -U {role} -h localhost -p 5432 -d {db_name} < /tmp/{name}.dump`
3. Применяем миграции на локальной БД.

## Code style
В каждом проекте существует договоренность по стилю. Вся эта договоренность сводится к запуску этой команды:
```bash
make format
```

Но для этого надо иметь на компьютере `make`

Если его нет, то запустите это:
```bash
isort ./<app_name>
black ./<app_name>
isort ./tests
black ./tests
isort ./migrations
black ./migrations
```

`<app_name>` - это папка с кодом, в которой лежит весь проект.

## Naming Convention
Переменные, модули и функции мы называем в формате `lower_case`

Классы мы называем в формате `CamelCase`

Более подробный гайд можно найти тут: <https://peps.python.org/pep-0008/#naming-conventions>

## Полезные ссылки и источники информации
- <https://pydantic-docs.helpmanual.io>
- <https://fastapi.tiangolo.com>
- <https://docs.sqlalchemy.org/en/20/>
- <https://docs.pytest.org/en/7.1.x/contents.html>
